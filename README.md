# pyPochodnia

Biblioteka do tworzenia grafÃ³w obliczeniowych z automatycznym rÃ³Å¼niczkowaniem (autograd) dla sieci neuronowych MLP w czystym NumPy.

## Cel projektu

pyPochodnia to minimalistyczna implementacja frameworka do deep learningu, podobna do PyTorch, ale napisana od podstaw w Pythonie i NumPy. Projekt umoÅ¼liwia:

- Tworzenie grafÃ³w obliczeniowych
- Automatyczne rÃ³Å¼niczkowanie (backward propagation)
- Budowanie i trenowanie sieci MLP
- EdukacjÄ™ na temat jak dziaÅ‚ajÄ… frameworki deep learningowe

## Struktura projektu

```
pyPochodnia/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ node/                    # Podstawowe wÄ™zÅ‚y grafu
â”‚   â”‚   â”œâ”€â”€ node.py             # Klasa bazowa Node
â”‚   â”‚   â”œâ”€â”€ constant.py         # WÄ™zeÅ‚ staÅ‚ej
â”‚   â”‚   â”œâ”€â”€ variable.py         # WÄ™zeÅ‚ zmiennej
â”‚   â”‚   â””â”€â”€ operations/         # Operacje
â”‚   â”‚       â”œâ”€â”€ arithmetic/     # Operacje arytmetyczne
â”‚   â”‚       â”‚   â”œâ”€â”€ add.py      # Dodawanie
â”‚   â”‚       â”‚   â”œâ”€â”€ subtract.py # Odejmowanie
â”‚   â”‚       â”‚   â”œâ”€â”€ multiply.py # MnoÅ¼enie
â”‚   â”‚       â”‚   â”œâ”€â”€ divide.py   # Dzielenie
â”‚   â”‚       â”‚   â”œâ”€â”€ power.py    # PotÄ™gowanie
â”‚   â”‚       â”‚   â””â”€â”€ matmul.py   # MnoÅ¼enie macierzowe
â”‚   â”‚       â”œâ”€â”€ activation.py   # Funkcje aktywacji (ReLU, Sigmoid, Tanh, Softmax)
â”‚   â”‚       â””â”€â”€ loss.py         # Funkcje straty (MSE, CrossEntropy)
â”‚   â”œâ”€â”€ layers/                  # Warstwy sieci
â”‚   â”‚   â””â”€â”€ dense.py            # Warstwa fully-connected
â”‚   â”œâ”€â”€ models/                  # Modele
â”‚   â”‚   â””â”€â”€ mlp.py              # Multi-Layer Perceptron
â”‚   â””â”€â”€ optimizers/              # Optymalizatory
â”‚       â””â”€â”€ optimizer.py        # SGD, Adam
â”œâ”€â”€ examples/                    # PrzykÅ‚ady uÅ¼ycia
â”‚   â”œâ”€â”€ example_mlp_xor.py      # Problem XOR
â”‚   â””â”€â”€ example_mlp_regression.py # Regresja liniowa
â”œâ”€â”€ tests/                       # Testy jednostkowe
â”‚   â”œâ”€â”€ test_nodes.py           # Testy wÄ™zÅ‚Ã³w
â”‚   â””â”€â”€ test_arithmetic_operations.py # Testy operacji
â””â”€â”€ main.py                      # GÅ‚Ã³wny plik
```

## ðŸš€ Instalacja

```bash
# Klonowanie repozytorium
git clone https://github.com/yourusername/pyPochodnia.git
cd pyPochodnia

# Instalacja zaleÅ¼noÅ›ci
pip install numpy pandas pydantic pytest
```

## PrzykÅ‚ady uÅ¼ycia

### Podstawowe operacje

```python
import numpy as np
from app.node import Variable, Constant
from app.node.operations.arithmetic import Add, Multiply

# Tworzenie zmiennych
x = Variable(value=np.array([1.0, 2.0, 3.0]), requires_grad=True)
w = Variable(value=np.array([2.0, 2.0, 2.0]), requires_grad=True)
b = Constant(value=np.array([1.0, 1.0, 1.0]))

# Budowanie grafu: y = x * w + b
mul_node = Multiply(x, w)
result = Add(mul_node, b)

# Forward pass
output = result.forward()
print(f"Output: {output}")

# Backward pass
result.backward()
print(f"Gradient x: {x.grad}")
print(f"Gradient w: {w.grad}")
```

### Uruchomienie przykÅ‚adÃ³w

```bash
# Problem XOR
python examples/example_mlp_xor.py

# Regresja liniowa
python examples/example_mlp_regression.py
```

## DostÄ™pne komponenty

### WÄ™zÅ‚y (Nodes)
- **Variable**: WÄ™zeÅ‚ przechowujÄ…cy dane z opcjonalnym gradientem
- **Constant**: WÄ™zeÅ‚ staÅ‚ej (bez gradientu)

### Operacje arytmetyczne
- **Add**: Dodawanie (a + b)
- **Subtract**: Odejmowanie (a - b)
- **Multiply**: MnoÅ¼enie element-wise (a * b)
- **Divide**: Dzielenie (a / b)
- **Power**: PotÄ™gowanie (a^b)
- **MatMul**: MnoÅ¼enie macierzowe (a @ b)

### Funkcje aktywacji
- **ReLU**: f(x) = max(0, x)
- **Sigmoid**: f(x) = 1 / (1 + exp(-x))
- **Tanh**: f(x) = tanh(x)
- **Softmax**: Normalizacja prawdopodobieÅ„stw

### Funkcje straty
- **MSELoss**: Mean Squared Error
- **CrossEntropyLoss**: Cross Entropy dla klasyfikacji wieloklasowej
- **BinaryCrossEntropyLoss**: Binary Cross Entropy dla klasyfikacji binarnej

### Warstwy
- **Dense**: Warstwa fully-connected (linear) z opcjonalnym bias

### Modele
- **MLP**: Multi-Layer Perceptron z konfigurowalnymi warstwami i aktywacjami

### Optymalizatory
- **SGD**: Stochastic Gradient Descent (z opcjonalnym momentum)
- **Adam**: Adaptive Moment Estimation

